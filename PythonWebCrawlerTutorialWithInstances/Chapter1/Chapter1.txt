Chapter1: Overview of Web Crawlers
1.1 Getting to Know Web Crawlers
1.1.1 The Meaning of Web Crawler
1.web crawler == a program whose aim is to download the web page on the Internet to local places and extract related data.

2.There are 2 aims of web crawler: one is to download target web pages, the other is to extract the needed data from target web pages.

1.1.2 The Main Types of Web Crawlers
1.General Purpose Crawler == also known as "Scalable Web Crawler", crawling objects from some seed URLs to the entire Web, it collects data mainly for the portal site, search engines and large Web service providers.

2.Focused Crawler == web crawlers that selectively crawl pages related to pre-defined topics.

3.Incremental Web Crawler == Crawlers that incrementally update downloaded pages and crawl only newly generated or changed pages.

4.Deep Web Crawler
	* Web pages can be divided into 2 categories: Surface Web & Deep Web

	(1)Surface Web == web pages that can be indexed by traditional search engines and are mainly composed of static pages that can be reached by hyperlinks.

	(2)Deep Web == web pages where most of the content is not available through static links, hidden behind search forms, and only available if the user submits some keywords.

	For example, the web pages that are visible only after the users have enrolled are Deep Webs.

1.1.3 The Structure of Simple Web Crawlers
1.There are 4 parts
	(1)URL Manager == manage URLs to be crawled to prevent duplicate and circular crawling.

	(2)Web Pages Downloader == the component that downloads web pages, it can be used to download web pages corresponding to URLs on the Internet to the local, it is one of the core parts of web crawlers.

	(3)Page Parser == the component that parses a web page to extract valuable data from the page, it is another core part of web crawlers.

	(4)Output Manager == a component that saves information and is used to output the parsed content to a file or database.

1.2 Overview of Python Web Crawling Technologies
1.2.1 Implementing HTTP Requests in Python
1.There are 2 libs to implement HTTP requests
	(1)urllib == Python's built-in HTTP request library, which can be called directly.

	(2)Requests lib == Based on urllib's open source HTTP library and it is more convenient to use.

1.2.2 Implementing Web Parsing in Python
1.There are 3 tools to use:
	(1)Regular Expression == describes a string matching pattern that can be used to check whether a string contains a certain substring, replace the matched substring or take out a substring from a string that meets a certain condition, etc.

	(2)Lxml lib == uses XPath grammar.
	[1]XPath == a language that searchs information in XML docs.

	(3)Beautiful Soup == a Python lib that can extract data from HTML or XML docs.

1.2.3 Web Crawler Framework
1.Web Crawler Framework == allows to call the framework's interface according to the specific project, you can write a small amount of code to implement a crawler.

2.There are mainly 3 types framework
(1)Scrapy == relatively more mature.

(2)pyspider == scripting, scheduling and crawling results of the real-time view in the browser interface, the back-end use of commonly used databases to crawl the results of the storage, but also to set up a regular task and task priorities, etc.

(3)Cola == a distributed crawler framework, the user only need to write a few specific functions, without having to pay attention to the details of the distributed operation, the task will be automatically assigned to multiple machines, the entire process is transparent to the user.
